{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/l-guilherme/PLN/blob/main/PLN_T2_ner_crf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nKhTfZUM7i21",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c78b79d3-ed57-4389-9b8c-09b5c28605d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (24.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-25.0-py3-none-any.whl.metadata (3.7 kB)\n",
            "Downloading pip-25.0-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 24.1.2\n",
            "    Uninstalling pip-24.1.2:\n",
            "      Successfully uninstalled pip-24.1.2\n",
            "Successfully installed pip-25.0\n",
            "Collecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from seqeval) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.11/dist-packages (from seqeval) (1.6.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.5.0)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16161 sha256=7bbf18c9d864752db3a8cfbec01d78ff9aa7d97dd451f2991192c7659d64a3c3\n",
            "  Stored in directory: /root/.cache/pip/wheels/bc/92/f0/243288f899c2eacdfa8c5f9aede4c71a9bad0ee26a01dc5ead\n",
            "Successfully built seqeval\n",
            "Installing collected packages: seqeval\n",
            "Successfully installed seqeval-1.2.2\n",
            "Collecting sklearn_crfsuite\n",
            "  Downloading sklearn_crfsuite-0.5.0-py2.py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting python-crfsuite>=0.9.7 (from sklearn_crfsuite)\n",
            "  Downloading python_crfsuite-0.9.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: scikit-learn>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from sklearn_crfsuite) (1.6.1)\n",
            "Requirement already satisfied: tabulate>=0.4.2 in /usr/local/lib/python3.11/dist-packages (from sklearn_crfsuite) (0.9.0)\n",
            "Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.11/dist-packages (from sklearn_crfsuite) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.24.0->sklearn_crfsuite) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.24.0->sklearn_crfsuite) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.24.0->sklearn_crfsuite) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.24.0->sklearn_crfsuite) (3.5.0)\n",
            "Downloading sklearn_crfsuite-0.5.0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading python_crfsuite-0.9.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-crfsuite, sklearn_crfsuite\n",
            "Successfully installed python-crfsuite-0.9.11 sklearn_crfsuite-0.5.0\n",
            "Collecting scikit-learn==1.3.2\n",
            "  Downloading scikit_learn-1.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17.3 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.3.2) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.3.2) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.3.2) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.3.2) (3.5.0)\n",
            "Downloading scikit_learn-1.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m116.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scikit-learn\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.6.1\n",
            "    Uninstalling scikit-learn-1.6.1:\n",
            "      Successfully uninstalled scikit-learn-1.6.1\n",
            "Successfully installed scikit-learn-1.3.2\n",
            "Collecting pt-core-news-sm==3.7.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/pt_core_news_sm-3.7.0/pt_core_news_sm-3.7.0-py3-none-any.whl (13.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m72.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from pt-core-news-sm==3.7.0) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (0.15.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2.10.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (3.1.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2024.12.14)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (0.1.2)\n",
            "Installing collected packages: pt-core-news-sm\n",
            "Successfully installed pt-core-news-sm-3.7.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('pt_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "!python -m pip install --upgrade pip\n",
        "!pip install seqeval\n",
        "!pip install -U sklearn_crfsuite\n",
        "!pip install scikit-learn==1.3.2\n",
        "\n",
        "!python -m spacy download pt_core_news_sm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "from sklearn_crfsuite import CRF\n",
        "from seqeval.metrics import classification_report"
      ],
      "metadata": {
        "id": "48UaJarT7rhN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/messias077/ner_pt/main/data/corpora/le_ner/train.conll -P le_ner/\n",
        "!wget https://raw.githubusercontent.com/messias077/ner_pt/main/data/corpora/le_ner/test.conll -P le_ner/\n",
        "\n",
        "!wget https://raw.githubusercontent.com/ulysses-camara/ulysses-ner-br/refs/heads/main/PL-corpus_v1/pl_corpus_categorias/train.txt -P ulysses_ner/\n",
        "!wget https://raw.githubusercontent.com/ulysses-camara/ulysses-ner-br/refs/heads/main/PL-corpus_v1/pl_corpus_categorias/test.txt -P ulysses_ner/"
      ],
      "metadata": {
        "id": "hPP-n9xN76Jz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c66781ef-e0cc-4991-8599-a4098d33ac47"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-03 10:32:57--  https://raw.githubusercontent.com/messias077/ner_pt/main/data/corpora/le_ner/train.conll\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2142199 (2.0M) [text/plain]\n",
            "Saving to: ‘le_ner/train.conll’\n",
            "\n",
            "train.conll         100%[===================>]   2.04M  --.-KB/s    in 0.03s   \n",
            "\n",
            "2025-02-03 10:32:57 (77.5 MB/s) - ‘le_ner/train.conll’ saved [2142199/2142199]\n",
            "\n",
            "--2025-02-03 10:32:58--  https://raw.githubusercontent.com/messias077/ner_pt/main/data/corpora/le_ner/test.conll\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 438441 (428K) [text/plain]\n",
            "Saving to: ‘le_ner/test.conll’\n",
            "\n",
            "test.conll          100%[===================>] 428.17K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2025-02-03 10:32:58 (23.4 MB/s) - ‘le_ner/test.conll’ saved [438441/438441]\n",
            "\n",
            "--2025-02-03 10:32:58--  https://raw.githubusercontent.com/ulysses-camara/ulysses-ner-br/refs/heads/main/PL-corpus_v1/pl_corpus_categorias/train.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 779304 (761K) [text/plain]\n",
            "Saving to: ‘ulysses_ner/train.txt’\n",
            "\n",
            "train.txt           100%[===================>] 761.04K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2025-02-03 10:32:58 (35.2 MB/s) - ‘ulysses_ner/train.txt’ saved [779304/779304]\n",
            "\n",
            "--2025-02-03 10:32:58--  https://raw.githubusercontent.com/ulysses-camara/ulysses-ner-br/refs/heads/main/PL-corpus_v1/pl_corpus_categorias/test.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 185614 (181K) [text/plain]\n",
            "Saving to: ‘ulysses_ner/test.txt’\n",
            "\n",
            "test.txt            100%[===================>] 181.26K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2025-02-03 10:32:59 (14.4 MB/s) - ‘ulysses_ner/test.txt’ saved [185614/185614]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Função para leitura da base de dados no padrão BIO"
      ],
      "metadata": {
        "id": "pK8x3LXd8BJU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_corpus_file(corpus_file, delimiter='\\t', ner_column=1):\n",
        "    with open(corpus_file, encoding='utf-8') as file:\n",
        "        lines = file.readlines()\n",
        "    data = []\n",
        "    words = []\n",
        "    tags = []\n",
        "    for line in lines:\n",
        "        line = line.replace('\\n', '')\n",
        "        if line != '':\n",
        "            if delimiter in line:\n",
        "                fragments = line.split(delimiter)\n",
        "                words.append(fragments[0])\n",
        "                tags.append(fragments[ner_column])\n",
        "        else:\n",
        "            if len(words) > 1:\n",
        "                data.append((words, tags))\n",
        "            words = []\n",
        "            tags = []\n",
        "    return data"
      ],
      "metadata": {
        "id": "1r7G62h77-pS"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_name = 'le_ner'\n",
        "#corpus_name = 'ulysses_ner'\n",
        "\n",
        "report_dir = 'report/'\n",
        "\n",
        "train_file = None\n",
        "test_file = None\n",
        "\n",
        "id_ner = 1\n",
        "delimiter = ' '\n",
        "\n",
        "if corpus_name == 'le_ner':\n",
        "  train_file = '/content/le_ner/train.conll'\n",
        "  test_file = '/content/le_ner/test.conll'\n",
        "elif corpus_name == 'ulysses_ner':\n",
        "  train_file = '/content/ulysses_ner/train.txt'\n",
        "  test_file = '/content/ulysses_ner/test.txt'\n",
        "\n",
        "print(f'\\nCorpus: {corpus_name}')\n",
        "\n",
        "report_dir = os.path.join(report_dir, corpus_name)\n",
        "\n",
        "os.makedirs(report_dir, exist_ok=True)\n",
        "\n",
        "train_data = read_corpus_file(train_file, delimiter=delimiter, ner_column=id_ner)\n",
        "test_data = read_corpus_file(test_file, delimiter=delimiter, ner_column=id_ner)\n",
        "\n",
        "print(f'\\nTrain data: {len(train_data)}')\n",
        "print(f'Test data: {len(test_data)}')\n",
        "\n",
        "test_data_original = np.array(test_data, dtype=object)"
      ],
      "metadata": {
        "id": "JgJM7-9z8GdK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2bd0e72-a747-410d-874a-10e92dc8b96d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Corpus: le_ner\n",
            "\n",
            "Train data: 7821\n",
            "Test data: 1389\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data[0]"
      ],
      "metadata": {
        "id": "-AG4Yri0Wr4b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de7a4dc9-1d69-42e6-ebac-5c9083db261d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['EMENTA',\n",
              "  ':',\n",
              "  'APELAÇÃO',\n",
              "  'CÍVEL',\n",
              "  '-',\n",
              "  'AÇÃO',\n",
              "  'DE',\n",
              "  'INDENIZAÇÃO',\n",
              "  'POR',\n",
              "  'DANOS',\n",
              "  'MORAIS',\n",
              "  '-',\n",
              "  'PRELIMINAR',\n",
              "  '-',\n",
              "  'ARGUIDA',\n",
              "  'PELO',\n",
              "  'MINISTÉRIO',\n",
              "  'PÚBLICO',\n",
              "  'EM',\n",
              "  'GRAU',\n",
              "  'RECURSAL',\n",
              "  '-',\n",
              "  'NULIDADE',\n",
              "  '-',\n",
              "  'AUSÊNCIA',\n",
              "  'DE',\n",
              "  'INTERVENÇÃO',\n",
              "  'DO',\n",
              "  'PARQUET',\n",
              "  'NA',\n",
              "  'INSTÂNCIA',\n",
              "  'A',\n",
              "  'QUO',\n",
              "  '-',\n",
              "  'PRESENÇA',\n",
              "  'DE',\n",
              "  'INCAPAZ',\n",
              "  '-',\n",
              "  'PREJUÍZO',\n",
              "  'EXISTENTE',\n",
              "  '-',\n",
              "  'PRELIMINAR',\n",
              "  'ACOLHIDA',\n",
              "  '-',\n",
              "  'NULIDADE',\n",
              "  'RECONHECIDA',\n",
              "  '.'],\n",
              " ['O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'B-ORGANIZACAO',\n",
              "  'I-ORGANIZACAO',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O'])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Função que executa o pré-processamento do corpus usando a ferramenta Spacy"
      ],
      "metadata": {
        "id": "7ValkLyP8OXK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "def data_preprocessing(data):\n",
        "    nlp = spacy.load(name='pt_core_news_sm',\n",
        "                     disable=['parser', 'ner', 'lemmatizer', 'textcat'])\n",
        "    preprocessed_data = []\n",
        "    for d in data:\n",
        "        sentence = ' '.join(d[0])\n",
        "        doc = nlp(sentence)\n",
        "        pos_tags = [t.pos_ for t in doc]\n",
        "        preprocessed_data.append((d[0], pos_tags, d[1]))\n",
        "    return preprocessed_data"
      ],
      "metadata": {
        "id": "rAKyUmPa8K7q"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = data_preprocessing(train_data)\n",
        "\n",
        "test_data = data_preprocessing(test_data)"
      ],
      "metadata": {
        "id": "NZMHKGET8Vgp"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data[0]"
      ],
      "metadata": {
        "id": "KVwOPdvoYM4y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93725f9b-b9bd-4e97-d080-6857ea580687"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['EMENTA',\n",
              "  ':',\n",
              "  'APELAÇÃO',\n",
              "  'CÍVEL',\n",
              "  '-',\n",
              "  'AÇÃO',\n",
              "  'DE',\n",
              "  'INDENIZAÇÃO',\n",
              "  'POR',\n",
              "  'DANOS',\n",
              "  'MORAIS',\n",
              "  '-',\n",
              "  'PRELIMINAR',\n",
              "  '-',\n",
              "  'ARGUIDA',\n",
              "  'PELO',\n",
              "  'MINISTÉRIO',\n",
              "  'PÚBLICO',\n",
              "  'EM',\n",
              "  'GRAU',\n",
              "  'RECURSAL',\n",
              "  '-',\n",
              "  'NULIDADE',\n",
              "  '-',\n",
              "  'AUSÊNCIA',\n",
              "  'DE',\n",
              "  'INTERVENÇÃO',\n",
              "  'DO',\n",
              "  'PARQUET',\n",
              "  'NA',\n",
              "  'INSTÂNCIA',\n",
              "  'A',\n",
              "  'QUO',\n",
              "  '-',\n",
              "  'PRESENÇA',\n",
              "  'DE',\n",
              "  'INCAPAZ',\n",
              "  '-',\n",
              "  'PREJUÍZO',\n",
              "  'EXISTENTE',\n",
              "  '-',\n",
              "  'PRELIMINAR',\n",
              "  'ACOLHIDA',\n",
              "  '-',\n",
              "  'NULIDADE',\n",
              "  'RECONHECIDA',\n",
              "  '.'],\n",
              " ['PROPN',\n",
              "  'PUNCT',\n",
              "  'PROPN',\n",
              "  'PROPN',\n",
              "  'PUNCT',\n",
              "  'PROPN',\n",
              "  'ADP',\n",
              "  'PROPN',\n",
              "  'ADP',\n",
              "  'PROPN',\n",
              "  'PROPN',\n",
              "  'PUNCT',\n",
              "  'PROPN',\n",
              "  'PUNCT',\n",
              "  'VERB',\n",
              "  'ADP',\n",
              "  'PROPN',\n",
              "  'PROPN',\n",
              "  'ADP',\n",
              "  'PROPN',\n",
              "  'PROPN',\n",
              "  'PUNCT',\n",
              "  'PROPN',\n",
              "  'PUNCT',\n",
              "  'X',\n",
              "  'ADP',\n",
              "  'PROPN',\n",
              "  'ADP',\n",
              "  'PROPN',\n",
              "  'ADP',\n",
              "  'NOUN',\n",
              "  'DET',\n",
              "  'PROPN',\n",
              "  'PUNCT',\n",
              "  'PROPN',\n",
              "  'ADP',\n",
              "  'PROPN',\n",
              "  'PUNCT',\n",
              "  'PROPN',\n",
              "  'PROPN',\n",
              "  'PUNCT',\n",
              "  'PROPN',\n",
              "  'PROPN',\n",
              "  'PUNCT',\n",
              "  'PROPN',\n",
              "  'NOUN',\n",
              "  'PUNCT'],\n",
              " ['O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'B-ORGANIZACAO',\n",
              "  'I-ORGANIZACAO',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O'])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Funções usadas para extrair as features dos tokens e de seus vizinhos."
      ],
      "metadata": {
        "id": "5hcAEkI08Yo0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_sent_features(sentence):\n",
        "    return [extract_features(sentence, i) for i in range(len(sentence))]\n",
        "\n",
        "\n",
        "def extract_labels(sentence):\n",
        "    return [label for _, _, label in sentence]\n",
        "\n",
        "\n",
        "def extract_features(sentence, i):\n",
        "    word = sentence[i][0]\n",
        "    postag = sentence[i][1]\n",
        "    features = {\n",
        "        'bias': 1.0,\n",
        "        'word.lower()': word.lower(),\n",
        "        'word[-3:]': word[-3:],\n",
        "        'word[-2:]': word[-2:],\n",
        "        'word.isupper()': word.isupper(),\n",
        "        'word.istitle()': word.istitle(),\n",
        "        'word.isdigit()': word.isdigit(),\n",
        "        'postag': postag,\n",
        "        'postag[:2]': postag[:2],\n",
        "        'word.islower()': word.islower(),\n",
        "        'word[0].isupper()': word[0].isupper(),\n",
        "        'word[0].islower()': word[0].islower(),\n",
        "        'not word[0].isalnum()': not word[0].isalnum(),\n",
        "        'not word.isalnum()': not word.isalnum(),\n",
        "        'word.isalpha()': word.isalpha()\n",
        "    }\n",
        "    if i > 0:\n",
        "        word1 = sentence[i - 1][0]\n",
        "        postag1 = sentence[i - 1][1]\n",
        "        features.update({\n",
        "            '-1:word.lower()': word1.lower(),\n",
        "            '-1:word.istitle()': word1.istitle(),\n",
        "            '-1:word.isupper()': word1.isupper(),\n",
        "            '-1:postag': postag1,\n",
        "            '-1:postag[:2]': postag1[:2],\n",
        "            '-1:word.islower()': word1.islower()\n",
        "        })\n",
        "    else:\n",
        "        features['BOS'] = True # BOS = Begin of Sentence\n",
        "    if i > 1:\n",
        "        word1 = sentence[i - 2][0]\n",
        "        postag1 = sentence[i - 2][1]\n",
        "        features.update({\n",
        "            '-2:word.lower()': word1.lower(),\n",
        "            '-2:word.istitle()': word1.istitle(),\n",
        "            '-2:word.isupper()': word1.isupper(),\n",
        "            '-2:postag': postag1,\n",
        "            '-2:postag[:2]': postag1[:2],\n",
        "            '-2:word.islower()': word1.islower()\n",
        "        })\n",
        "    if i < len(sentence) - 1:\n",
        "        word1 = sentence[i + 1][0]\n",
        "        postag1 = sentence[i + 1][1]\n",
        "        features.update({\n",
        "            '+1:word.lower()': word1.lower(),\n",
        "            '+1:word.istitle()': word1.istitle(),\n",
        "            '+1:word.isupper()': word1.isupper(),\n",
        "            '+1:postag': postag1,\n",
        "            '+1:postag[:2]': postag1[:2],\n",
        "            '+1:word.islower()': word1.islower()\n",
        "        })\n",
        "    else:\n",
        "        features['EOS'] = True # EOS = End of Sentence\n",
        "    if i < len(sentence) - 2:\n",
        "        word1 = sentence[i + 2][0]\n",
        "        postag1 = sentence[i + 2][1]\n",
        "        features.update({\n",
        "            '+2:word.lower()': word1.lower(),\n",
        "            '+2:word.istitle()': word1.istitle(),\n",
        "            '+2:word.isupper()': word1.isupper(),\n",
        "            '+2:postag': postag1,\n",
        "            '+2:postag[:2]': postag1[:2],\n",
        "            '+2:word.islower()': word1.islower()\n",
        "        })\n",
        "    return features\n",
        "\n",
        "\n",
        "def convert_data(data):\n",
        "    sentences = []\n",
        "    for d in data:\n",
        "        sentences.append(list(zip(d[0], d[1], d[2])))\n",
        "    x_data = [extract_sent_features(s) for s in sentences]\n",
        "    y_data = [extract_labels(s) for s in sentences]\n",
        "    return x_data, y_data"
      ],
      "metadata": {
        "id": "vRvHiHzO8gj7"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train = convert_data(train_data)\n",
        "\n",
        "X_test, y_test = convert_data(test_data)"
      ],
      "metadata": {
        "id": "iGcxsxO78kYj"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'\\nExample features: {X_train[0]}')\n",
        "\n",
        "print(f'\\nLabel: {y_train[0]}')"
      ],
      "metadata": {
        "id": "cKwDtpC48nmS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "053d1d4b-7c8d-4595-afc5-e294c25d7c54"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Example features: [{'bias': 1.0, 'word.lower()': 'ementa', 'word[-3:]': 'NTA', 'word[-2:]': 'TA', 'word.isupper()': True, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'PROPN', 'postag[:2]': 'PR', 'word.islower()': False, 'word[0].isupper()': True, 'word[0].islower()': False, 'not word[0].isalnum()': False, 'not word.isalnum()': False, 'word.isalpha()': True, 'BOS': True, '+1:word.lower()': ':', '+1:word.istitle()': False, '+1:word.isupper()': False, '+1:postag': 'PUNCT', '+1:postag[:2]': 'PU', '+1:word.islower()': False, '+2:word.lower()': 'apelação', '+2:word.istitle()': False, '+2:word.isupper()': True, '+2:postag': 'PROPN', '+2:postag[:2]': 'PR', '+2:word.islower()': False}, {'bias': 1.0, 'word.lower()': ':', 'word[-3:]': ':', 'word[-2:]': ':', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'PUNCT', 'postag[:2]': 'PU', 'word.islower()': False, 'word[0].isupper()': False, 'word[0].islower()': False, 'not word[0].isalnum()': True, 'not word.isalnum()': True, 'word.isalpha()': False, '-1:word.lower()': 'ementa', '-1:word.istitle()': False, '-1:word.isupper()': True, '-1:postag': 'PROPN', '-1:postag[:2]': 'PR', '-1:word.islower()': False, '+1:word.lower()': 'apelação', '+1:word.istitle()': False, '+1:word.isupper()': True, '+1:postag': 'PROPN', '+1:postag[:2]': 'PR', '+1:word.islower()': False, '+2:word.lower()': 'cível', '+2:word.istitle()': False, '+2:word.isupper()': True, '+2:postag': 'PROPN', '+2:postag[:2]': 'PR', '+2:word.islower()': False}, {'bias': 1.0, 'word.lower()': 'apelação', 'word[-3:]': 'ÇÃO', 'word[-2:]': 'ÃO', 'word.isupper()': True, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'PROPN', 'postag[:2]': 'PR', 'word.islower()': False, 'word[0].isupper()': True, 'word[0].islower()': False, 'not word[0].isalnum()': False, 'not word.isalnum()': False, 'word.isalpha()': True, '-1:word.lower()': ':', '-1:word.istitle()': False, '-1:word.isupper()': False, '-1:postag': 'PUNCT', '-1:postag[:2]': 'PU', '-1:word.islower()': False, '-2:word.lower()': 'ementa', '-2:word.istitle()': False, '-2:word.isupper()': True, '-2:postag': 'PROPN', '-2:postag[:2]': 'PR', '-2:word.islower()': False, '+1:word.lower()': 'cível', '+1:word.istitle()': False, '+1:word.isupper()': True, '+1:postag': 'PROPN', '+1:postag[:2]': 'PR', '+1:word.islower()': False, '+2:word.lower()': '-', '+2:word.istitle()': False, '+2:word.isupper()': False, '+2:postag': 'PUNCT', '+2:postag[:2]': 'PU', '+2:word.islower()': False}, {'bias': 1.0, 'word.lower()': 'cível', 'word[-3:]': 'VEL', 'word[-2:]': 'EL', 'word.isupper()': True, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'PROPN', 'postag[:2]': 'PR', 'word.islower()': False, 'word[0].isupper()': True, 'word[0].islower()': False, 'not word[0].isalnum()': False, 'not word.isalnum()': False, 'word.isalpha()': True, '-1:word.lower()': 'apelação', '-1:word.istitle()': False, '-1:word.isupper()': True, '-1:postag': 'PROPN', '-1:postag[:2]': 'PR', '-1:word.islower()': False, '-2:word.lower()': ':', '-2:word.istitle()': False, '-2:word.isupper()': False, '-2:postag': 'PUNCT', '-2:postag[:2]': 'PU', '-2:word.islower()': False, '+1:word.lower()': '-', '+1:word.istitle()': False, '+1:word.isupper()': False, '+1:postag': 'PUNCT', '+1:postag[:2]': 'PU', '+1:word.islower()': False, '+2:word.lower()': 'ação', '+2:word.istitle()': False, '+2:word.isupper()': True, '+2:postag': 'PROPN', '+2:postag[:2]': 'PR', '+2:word.islower()': False}, {'bias': 1.0, 'word.lower()': '-', 'word[-3:]': '-', 'word[-2:]': '-', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'PUNCT', 'postag[:2]': 'PU', 'word.islower()': False, 'word[0].isupper()': False, 'word[0].islower()': False, 'not word[0].isalnum()': True, 'not word.isalnum()': True, 'word.isalpha()': False, '-1:word.lower()': 'cível', '-1:word.istitle()': False, '-1:word.isupper()': True, '-1:postag': 'PROPN', '-1:postag[:2]': 'PR', '-1:word.islower()': False, '-2:word.lower()': 'apelação', '-2:word.istitle()': False, '-2:word.isupper()': True, '-2:postag': 'PROPN', '-2:postag[:2]': 'PR', '-2:word.islower()': False, '+1:word.lower()': 'ação', '+1:word.istitle()': False, '+1:word.isupper()': True, '+1:postag': 'PROPN', '+1:postag[:2]': 'PR', '+1:word.islower()': False, '+2:word.lower()': 'de', '+2:word.istitle()': False, '+2:word.isupper()': True, '+2:postag': 'ADP', '+2:postag[:2]': 'AD', '+2:word.islower()': False}, {'bias': 1.0, 'word.lower()': 'ação', 'word[-3:]': 'ÇÃO', 'word[-2:]': 'ÃO', 'word.isupper()': True, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'PROPN', 'postag[:2]': 'PR', 'word.islower()': False, 'word[0].isupper()': True, 'word[0].islower()': False, 'not word[0].isalnum()': False, 'not word.isalnum()': False, 'word.isalpha()': True, '-1:word.lower()': '-', '-1:word.istitle()': False, '-1:word.isupper()': False, '-1:postag': 'PUNCT', '-1:postag[:2]': 'PU', '-1:word.islower()': False, '-2:word.lower()': 'cível', '-2:word.istitle()': False, '-2:word.isupper()': True, '-2:postag': 'PROPN', '-2:postag[:2]': 'PR', '-2:word.islower()': False, '+1:word.lower()': 'de', '+1:word.istitle()': False, '+1:word.isupper()': True, '+1:postag': 'ADP', '+1:postag[:2]': 'AD', '+1:word.islower()': False, '+2:word.lower()': 'indenização', '+2:word.istitle()': False, '+2:word.isupper()': True, '+2:postag': 'PROPN', '+2:postag[:2]': 'PR', '+2:word.islower()': False}, {'bias': 1.0, 'word.lower()': 'de', 'word[-3:]': 'DE', 'word[-2:]': 'DE', 'word.isupper()': True, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'ADP', 'postag[:2]': 'AD', 'word.islower()': False, 'word[0].isupper()': True, 'word[0].islower()': False, 'not word[0].isalnum()': False, 'not word.isalnum()': False, 'word.isalpha()': True, '-1:word.lower()': 'ação', '-1:word.istitle()': False, '-1:word.isupper()': True, '-1:postag': 'PROPN', '-1:postag[:2]': 'PR', '-1:word.islower()': False, '-2:word.lower()': '-', '-2:word.istitle()': False, '-2:word.isupper()': False, '-2:postag': 'PUNCT', '-2:postag[:2]': 'PU', '-2:word.islower()': False, '+1:word.lower()': 'indenização', '+1:word.istitle()': False, '+1:word.isupper()': True, '+1:postag': 'PROPN', '+1:postag[:2]': 'PR', '+1:word.islower()': False, '+2:word.lower()': 'por', '+2:word.istitle()': False, '+2:word.isupper()': True, '+2:postag': 'ADP', '+2:postag[:2]': 'AD', '+2:word.islower()': False}, {'bias': 1.0, 'word.lower()': 'indenização', 'word[-3:]': 'ÇÃO', 'word[-2:]': 'ÃO', 'word.isupper()': True, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'PROPN', 'postag[:2]': 'PR', 'word.islower()': False, 'word[0].isupper()': True, 'word[0].islower()': False, 'not word[0].isalnum()': False, 'not word.isalnum()': False, 'word.isalpha()': True, '-1:word.lower()': 'de', '-1:word.istitle()': False, '-1:word.isupper()': True, '-1:postag': 'ADP', '-1:postag[:2]': 'AD', '-1:word.islower()': False, '-2:word.lower()': 'ação', '-2:word.istitle()': False, '-2:word.isupper()': True, '-2:postag': 'PROPN', '-2:postag[:2]': 'PR', '-2:word.islower()': False, '+1:word.lower()': 'por', '+1:word.istitle()': False, '+1:word.isupper()': True, '+1:postag': 'ADP', '+1:postag[:2]': 'AD', '+1:word.islower()': False, '+2:word.lower()': 'danos', '+2:word.istitle()': False, '+2:word.isupper()': True, '+2:postag': 'PROPN', '+2:postag[:2]': 'PR', '+2:word.islower()': False}, {'bias': 1.0, 'word.lower()': 'por', 'word[-3:]': 'POR', 'word[-2:]': 'OR', 'word.isupper()': True, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'ADP', 'postag[:2]': 'AD', 'word.islower()': False, 'word[0].isupper()': True, 'word[0].islower()': False, 'not word[0].isalnum()': False, 'not word.isalnum()': False, 'word.isalpha()': True, '-1:word.lower()': 'indenização', '-1:word.istitle()': False, '-1:word.isupper()': True, '-1:postag': 'PROPN', '-1:postag[:2]': 'PR', '-1:word.islower()': False, '-2:word.lower()': 'de', '-2:word.istitle()': False, '-2:word.isupper()': True, '-2:postag': 'ADP', '-2:postag[:2]': 'AD', '-2:word.islower()': False, '+1:word.lower()': 'danos', '+1:word.istitle()': False, '+1:word.isupper()': True, '+1:postag': 'PROPN', '+1:postag[:2]': 'PR', '+1:word.islower()': False, '+2:word.lower()': 'morais', '+2:word.istitle()': False, '+2:word.isupper()': True, '+2:postag': 'PROPN', '+2:postag[:2]': 'PR', '+2:word.islower()': False}, {'bias': 1.0, 'word.lower()': 'danos', 'word[-3:]': 'NOS', 'word[-2:]': 'OS', 'word.isupper()': True, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'PROPN', 'postag[:2]': 'PR', 'word.islower()': False, 'word[0].isupper()': True, 'word[0].islower()': False, 'not word[0].isalnum()': False, 'not word.isalnum()': False, 'word.isalpha()': True, '-1:word.lower()': 'por', '-1:word.istitle()': False, '-1:word.isupper()': True, '-1:postag': 'ADP', '-1:postag[:2]': 'AD', '-1:word.islower()': False, '-2:word.lower()': 'indenização', '-2:word.istitle()': False, '-2:word.isupper()': True, '-2:postag': 'PROPN', '-2:postag[:2]': 'PR', '-2:word.islower()': False, '+1:word.lower()': 'morais', '+1:word.istitle()': False, '+1:word.isupper()': True, '+1:postag': 'PROPN', '+1:postag[:2]': 'PR', '+1:word.islower()': False, '+2:word.lower()': '-', '+2:word.istitle()': False, '+2:word.isupper()': False, '+2:postag': 'PUNCT', '+2:postag[:2]': 'PU', '+2:word.islower()': False}, {'bias': 1.0, 'word.lower()': 'morais', 'word[-3:]': 'AIS', 'word[-2:]': 'IS', 'word.isupper()': True, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'PROPN', 'postag[:2]': 'PR', 'word.islower()': False, 'word[0].isupper()': True, 'word[0].islower()': False, 'not word[0].isalnum()': False, 'not word.isalnum()': False, 'word.isalpha()': True, '-1:word.lower()': 'danos', '-1:word.istitle()': False, '-1:word.isupper()': True, '-1:postag': 'PROPN', '-1:postag[:2]': 'PR', '-1:word.islower()': False, '-2:word.lower()': 'por', '-2:word.istitle()': False, '-2:word.isupper()': True, '-2:postag': 'ADP', '-2:postag[:2]': 'AD', '-2:word.islower()': False, '+1:word.lower()': '-', '+1:word.istitle()': False, '+1:word.isupper()': False, '+1:postag': 'PUNCT', '+1:postag[:2]': 'PU', '+1:word.islower()': False, '+2:word.lower()': 'preliminar', '+2:word.istitle()': False, '+2:word.isupper()': True, '+2:postag': 'PROPN', '+2:postag[:2]': 'PR', '+2:word.islower()': False}, {'bias': 1.0, 'word.lower()': '-', 'word[-3:]': '-', 'word[-2:]': '-', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'PUNCT', 'postag[:2]': 'PU', 'word.islower()': False, 'word[0].isupper()': False, 'word[0].islower()': False, 'not word[0].isalnum()': True, 'not word.isalnum()': True, 'word.isalpha()': False, '-1:word.lower()': 'morais', '-1:word.istitle()': False, '-1:word.isupper()': True, '-1:postag': 'PROPN', '-1:postag[:2]': 'PR', '-1:word.islower()': False, '-2:word.lower()': 'danos', '-2:word.istitle()': False, '-2:word.isupper()': True, '-2:postag': 'PROPN', '-2:postag[:2]': 'PR', '-2:word.islower()': False, '+1:word.lower()': 'preliminar', '+1:word.istitle()': False, '+1:word.isupper()': True, '+1:postag': 'PROPN', '+1:postag[:2]': 'PR', '+1:word.islower()': False, '+2:word.lower()': '-', '+2:word.istitle()': False, '+2:word.isupper()': False, '+2:postag': 'PUNCT', '+2:postag[:2]': 'PU', '+2:word.islower()': False}, {'bias': 1.0, 'word.lower()': 'preliminar', 'word[-3:]': 'NAR', 'word[-2:]': 'AR', 'word.isupper()': True, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'PROPN', 'postag[:2]': 'PR', 'word.islower()': False, 'word[0].isupper()': True, 'word[0].islower()': False, 'not word[0].isalnum()': False, 'not word.isalnum()': False, 'word.isalpha()': True, '-1:word.lower()': '-', '-1:word.istitle()': False, '-1:word.isupper()': False, '-1:postag': 'PUNCT', '-1:postag[:2]': 'PU', '-1:word.islower()': False, '-2:word.lower()': 'morais', '-2:word.istitle()': False, '-2:word.isupper()': True, '-2:postag': 'PROPN', '-2:postag[:2]': 'PR', '-2:word.islower()': False, '+1:word.lower()': '-', '+1:word.istitle()': False, '+1:word.isupper()': False, '+1:postag': 'PUNCT', '+1:postag[:2]': 'PU', '+1:word.islower()': False, '+2:word.lower()': 'arguida', '+2:word.istitle()': False, '+2:word.isupper()': True, '+2:postag': 'VERB', '+2:postag[:2]': 'VE', '+2:word.islower()': False}, {'bias': 1.0, 'word.lower()': '-', 'word[-3:]': '-', 'word[-2:]': '-', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'PUNCT', 'postag[:2]': 'PU', 'word.islower()': False, 'word[0].isupper()': False, 'word[0].islower()': False, 'not word[0].isalnum()': True, 'not word.isalnum()': True, 'word.isalpha()': False, '-1:word.lower()': 'preliminar', '-1:word.istitle()': False, '-1:word.isupper()': True, '-1:postag': 'PROPN', '-1:postag[:2]': 'PR', '-1:word.islower()': False, '-2:word.lower()': '-', '-2:word.istitle()': False, '-2:word.isupper()': False, '-2:postag': 'PUNCT', '-2:postag[:2]': 'PU', '-2:word.islower()': False, '+1:word.lower()': 'arguida', '+1:word.istitle()': False, '+1:word.isupper()': True, '+1:postag': 'VERB', '+1:postag[:2]': 'VE', '+1:word.islower()': False, '+2:word.lower()': 'pelo', '+2:word.istitle()': False, '+2:word.isupper()': True, '+2:postag': 'ADP', '+2:postag[:2]': 'AD', '+2:word.islower()': False}, {'bias': 1.0, 'word.lower()': 'arguida', 'word[-3:]': 'IDA', 'word[-2:]': 'DA', 'word.isupper()': True, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'VERB', 'postag[:2]': 'VE', 'word.islower()': False, 'word[0].isupper()': True, 'word[0].islower()': False, 'not word[0].isalnum()': False, 'not word.isalnum()': False, 'word.isalpha()': True, '-1:word.lower()': '-', '-1:word.istitle()': False, '-1:word.isupper()': False, '-1:postag': 'PUNCT', '-1:postag[:2]': 'PU', '-1:word.islower()': False, '-2:word.lower()': 'preliminar', '-2:word.istitle()': False, '-2:word.isupper()': True, '-2:postag': 'PROPN', '-2:postag[:2]': 'PR', '-2:word.islower()': False, '+1:word.lower()': 'pelo', '+1:word.istitle()': False, '+1:word.isupper()': True, '+1:postag': 'ADP', '+1:postag[:2]': 'AD', '+1:word.islower()': False, '+2:word.lower()': 'ministério', '+2:word.istitle()': False, '+2:word.isupper()': True, '+2:postag': 'PROPN', '+2:postag[:2]': 'PR', '+2:word.islower()': False}, {'bias': 1.0, 'word.lower()': 'pelo', 'word[-3:]': 'ELO', 'word[-2:]': 'LO', 'word.isupper()': True, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'ADP', 'postag[:2]': 'AD', 'word.islower()': False, 'word[0].isupper()': True, 'word[0].islower()': False, 'not word[0].isalnum()': False, 'not word.isalnum()': False, 'word.isalpha()': True, '-1:word.lower()': 'arguida', '-1:word.istitle()': False, '-1:word.isupper()': True, '-1:postag': 'VERB', '-1:postag[:2]': 'VE', '-1:word.islower()': False, '-2:word.lower()': '-', '-2:word.istitle()': False, '-2:word.isupper()': False, '-2:postag': 'PUNCT', '-2:postag[:2]': 'PU', '-2:word.islower()': False, '+1:word.lower()': 'ministério', '+1:word.istitle()': False, '+1:word.isupper()': True, '+1:postag': 'PROPN', '+1:postag[:2]': 'PR', '+1:word.islower()': False, '+2:word.lower()': 'público', '+2:word.istitle()': False, '+2:word.isupper()': True, '+2:postag': 'PROPN', '+2:postag[:2]': 'PR', '+2:word.islower()': False}, {'bias': 1.0, 'word.lower()': 'ministério', 'word[-3:]': 'RIO', 'word[-2:]': 'IO', 'word.isupper()': True, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'PROPN', 'postag[:2]': 'PR', 'word.islower()': False, 'word[0].isupper()': True, 'word[0].islower()': False, 'not word[0].isalnum()': False, 'not word.isalnum()': False, 'word.isalpha()': True, '-1:word.lower()': 'pelo', '-1:word.istitle()': False, '-1:word.isupper()': True, '-1:postag': 'ADP', '-1:postag[:2]': 'AD', '-1:word.islower()': False, '-2:word.lower()': 'arguida', '-2:word.istitle()': False, '-2:word.isupper()': True, '-2:postag': 'VERB', '-2:postag[:2]': 'VE', '-2:word.islower()': False, '+1:word.lower()': 'público', '+1:word.istitle()': False, '+1:word.isupper()': True, '+1:postag': 'PROPN', '+1:postag[:2]': 'PR', '+1:word.islower()': False, '+2:word.lower()': 'em', '+2:word.istitle()': False, '+2:word.isupper()': True, '+2:postag': 'ADP', '+2:postag[:2]': 'AD', '+2:word.islower()': False}, {'bias': 1.0, 'word.lower()': 'público', 'word[-3:]': 'ICO', 'word[-2:]': 'CO', 'word.isupper()': True, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'PROPN', 'postag[:2]': 'PR', 'word.islower()': False, 'word[0].isupper()': True, 'word[0].islower()': False, 'not word[0].isalnum()': False, 'not word.isalnum()': False, 'word.isalpha()': True, '-1:word.lower()': 'ministério', '-1:word.istitle()': False, '-1:word.isupper()': True, '-1:postag': 'PROPN', '-1:postag[:2]': 'PR', '-1:word.islower()': False, '-2:word.lower()': 'pelo', '-2:word.istitle()': False, '-2:word.isupper()': True, '-2:postag': 'ADP', '-2:postag[:2]': 'AD', '-2:word.islower()': False, '+1:word.lower()': 'em', '+1:word.istitle()': False, '+1:word.isupper()': True, '+1:postag': 'ADP', '+1:postag[:2]': 'AD', '+1:word.islower()': False, '+2:word.lower()': 'grau', '+2:word.istitle()': False, '+2:word.isupper()': True, '+2:postag': 'PROPN', '+2:postag[:2]': 'PR', '+2:word.islower()': False}, {'bias': 1.0, 'word.lower()': 'em', 'word[-3:]': 'EM', 'word[-2:]': 'EM', 'word.isupper()': True, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'ADP', 'postag[:2]': 'AD', 'word.islower()': False, 'word[0].isupper()': True, 'word[0].islower()': False, 'not word[0].isalnum()': False, 'not word.isalnum()': False, 'word.isalpha()': True, '-1:word.lower()': 'público', '-1:word.istitle()': False, '-1:word.isupper()': True, '-1:postag': 'PROPN', '-1:postag[:2]': 'PR', '-1:word.islower()': False, '-2:word.lower()': 'ministério', '-2:word.istitle()': False, '-2:word.isupper()': True, '-2:postag': 'PROPN', '-2:postag[:2]': 'PR', '-2:word.islower()': False, '+1:word.lower()': 'grau', '+1:word.istitle()': False, '+1:word.isupper()': True, '+1:postag': 'PROPN', '+1:postag[:2]': 'PR', '+1:word.islower()': False, '+2:word.lower()': 'recursal', '+2:word.istitle()': False, '+2:word.isupper()': True, '+2:postag': 'PROPN', '+2:postag[:2]': 'PR', '+2:word.islower()': False}, {'bias': 1.0, 'word.lower()': 'grau', 'word[-3:]': 'RAU', 'word[-2:]': 'AU', 'word.isupper()': True, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'PROPN', 'postag[:2]': 'PR', 'word.islower()': False, 'word[0].isupper()': True, 'word[0].islower()': False, 'not word[0].isalnum()': False, 'not word.isalnum()': False, 'word.isalpha()': True, '-1:word.lower()': 'em', '-1:word.istitle()': False, '-1:word.isupper()': True, '-1:postag': 'ADP', '-1:postag[:2]': 'AD', '-1:word.islower()': False, '-2:word.lower()': 'público', '-2:word.istitle()': False, '-2:word.isupper()': True, '-2:postag': 'PROPN', '-2:postag[:2]': 'PR', '-2:word.islower()': False, '+1:word.lower()': 'recursal', '+1:word.istitle()': False, '+1:word.isupper()': True, '+1:postag': 'PROPN', '+1:postag[:2]': 'PR', '+1:word.islower()': False, '+2:word.lower()': '-', '+2:word.istitle()': False, '+2:word.isupper()': False, '+2:postag': 'PUNCT', '+2:postag[:2]': 'PU', '+2:word.islower()': False}, {'bias': 1.0, 'word.lower()': 'recursal', 'word[-3:]': 'SAL', 'word[-2:]': 'AL', 'word.isupper()': True, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'PROPN', 'postag[:2]': 'PR', 'word.islower()': False, 'word[0].isupper()': True, 'word[0].islower()': False, 'not word[0].isalnum()': False, 'not word.isalnum()': False, 'word.isalpha()': True, '-1:word.lower()': 'grau', '-1:word.istitle()': False, '-1:word.isupper()': True, '-1:postag': 'PROPN', '-1:postag[:2]': 'PR', '-1:word.islower()': False, '-2:word.lower()': 'em', '-2:word.istitle()': False, '-2:word.isupper()': True, '-2:postag': 'ADP', '-2:postag[:2]': 'AD', '-2:word.islower()': False, '+1:word.lower()': '-', '+1:word.istitle()': False, '+1:word.isupper()': False, '+1:postag': 'PUNCT', '+1:postag[:2]': 'PU', '+1:word.islower()': False, '+2:word.lower()': 'nulidade', '+2:word.istitle()': False, '+2:word.isupper()': True, '+2:postag': 'PROPN', '+2:postag[:2]': 'PR', '+2:word.islower()': False}, {'bias': 1.0, 'word.lower()': '-', 'word[-3:]': '-', 'word[-2:]': '-', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'PUNCT', 'postag[:2]': 'PU', 'word.islower()': False, 'word[0].isupper()': False, 'word[0].islower()': False, 'not word[0].isalnum()': True, 'not word.isalnum()': True, 'word.isalpha()': False, '-1:word.lower()': 'recursal', '-1:word.istitle()': False, '-1:word.isupper()': True, '-1:postag': 'PROPN', '-1:postag[:2]': 'PR', '-1:word.islower()': False, '-2:word.lower()': 'grau', '-2:word.istitle()': False, '-2:word.isupper()': True, '-2:postag': 'PROPN', '-2:postag[:2]': 'PR', '-2:word.islower()': False, '+1:word.lower()': 'nulidade', '+1:word.istitle()': False, '+1:word.isupper()': True, '+1:postag': 'PROPN', '+1:postag[:2]': 'PR', '+1:word.islower()': False, '+2:word.lower()': '-', '+2:word.istitle()': False, '+2:word.isupper()': False, '+2:postag': 'PUNCT', '+2:postag[:2]': 'PU', '+2:word.islower()': False}, {'bias': 1.0, 'word.lower()': 'nulidade', 'word[-3:]': 'ADE', 'word[-2:]': 'DE', 'word.isupper()': True, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'PROPN', 'postag[:2]': 'PR', 'word.islower()': False, 'word[0].isupper()': True, 'word[0].islower()': False, 'not word[0].isalnum()': False, 'not word.isalnum()': False, 'word.isalpha()': True, '-1:word.lower()': '-', '-1:word.istitle()': False, '-1:word.isupper()': False, '-1:postag': 'PUNCT', '-1:postag[:2]': 'PU', '-1:word.islower()': False, '-2:word.lower()': 'recursal', '-2:word.istitle()': False, '-2:word.isupper()': True, '-2:postag': 'PROPN', '-2:postag[:2]': 'PR', '-2:word.islower()': False, '+1:word.lower()': '-', '+1:word.istitle()': False, '+1:word.isupper()': False, '+1:postag': 'PUNCT', '+1:postag[:2]': 'PU', '+1:word.islower()': False, '+2:word.lower()': 'ausência', '+2:word.istitle()': False, '+2:word.isupper()': True, '+2:postag': 'X', '+2:postag[:2]': 'X', '+2:word.islower()': False}, {'bias': 1.0, 'word.lower()': '-', 'word[-3:]': '-', 'word[-2:]': '-', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'PUNCT', 'postag[:2]': 'PU', 'word.islower()': False, 'word[0].isupper()': False, 'word[0].islower()': False, 'not word[0].isalnum()': True, 'not word.isalnum()': True, 'word.isalpha()': False, '-1:word.lower()': 'nulidade', '-1:word.istitle()': False, '-1:word.isupper()': True, '-1:postag': 'PROPN', '-1:postag[:2]': 'PR', '-1:word.islower()': False, '-2:word.lower()': '-', '-2:word.istitle()': False, '-2:word.isupper()': False, '-2:postag': 'PUNCT', '-2:postag[:2]': 'PU', '-2:word.islower()': False, '+1:word.lower()': 'ausência', '+1:word.istitle()': False, '+1:word.isupper()': True, '+1:postag': 'X', '+1:postag[:2]': 'X', '+1:word.islower()': False, '+2:word.lower()': 'de', '+2:word.istitle()': False, '+2:word.isupper()': True, '+2:postag': 'ADP', '+2:postag[:2]': 'AD', '+2:word.islower()': False}, {'bias': 1.0, 'word.lower()': 'ausência', 'word[-3:]': 'CIA', 'word[-2:]': 'IA', 'word.isupper()': True, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'X', 'postag[:2]': 'X', 'word.islower()': False, 'word[0].isupper()': True, 'word[0].islower()': False, 'not word[0].isalnum()': False, 'not word.isalnum()': False, 'word.isalpha()': True, '-1:word.lower()': '-', '-1:word.istitle()': False, '-1:word.isupper()': False, '-1:postag': 'PUNCT', '-1:postag[:2]': 'PU', '-1:word.islower()': False, '-2:word.lower()': 'nulidade', '-2:word.istitle()': False, '-2:word.isupper()': True, '-2:postag': 'PROPN', '-2:postag[:2]': 'PR', '-2:word.islower()': False, '+1:word.lower()': 'de', '+1:word.istitle()': False, '+1:word.isupper()': True, '+1:postag': 'ADP', '+1:postag[:2]': 'AD', '+1:word.islower()': False, '+2:word.lower()': 'intervenção', '+2:word.istitle()': False, '+2:word.isupper()': True, '+2:postag': 'PROPN', '+2:postag[:2]': 'PR', '+2:word.islower()': False}, {'bias': 1.0, 'word.lower()': 'de', 'word[-3:]': 'DE', 'word[-2:]': 'DE', 'word.isupper()': True, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'ADP', 'postag[:2]': 'AD', 'word.islower()': False, 'word[0].isupper()': True, 'word[0].islower()': False, 'not word[0].isalnum()': False, 'not word.isalnum()': False, 'word.isalpha()': True, '-1:word.lower()': 'ausência', '-1:word.istitle()': False, '-1:word.isupper()': True, '-1:postag': 'X', '-1:postag[:2]': 'X', '-1:word.islower()': False, '-2:word.lower()': '-', '-2:word.istitle()': False, '-2:word.isupper()': False, '-2:postag': 'PUNCT', '-2:postag[:2]': 'PU', '-2:word.islower()': False, '+1:word.lower()': 'intervenção', '+1:word.istitle()': False, '+1:word.isupper()': True, '+1:postag': 'PROPN', '+1:postag[:2]': 'PR', '+1:word.islower()': False, '+2:word.lower()': 'do', '+2:word.istitle()': False, '+2:word.isupper()': True, '+2:postag': 'ADP', '+2:postag[:2]': 'AD', '+2:word.islower()': False}, {'bias': 1.0, 'word.lower()': 'intervenção', 'word[-3:]': 'ÇÃO', 'word[-2:]': 'ÃO', 'word.isupper()': True, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'PROPN', 'postag[:2]': 'PR', 'word.islower()': False, 'word[0].isupper()': True, 'word[0].islower()': False, 'not word[0].isalnum()': False, 'not word.isalnum()': False, 'word.isalpha()': True, '-1:word.lower()': 'de', '-1:word.istitle()': False, '-1:word.isupper()': True, '-1:postag': 'ADP', '-1:postag[:2]': 'AD', '-1:word.islower()': False, '-2:word.lower()': 'ausência', '-2:word.istitle()': False, '-2:word.isupper()': True, '-2:postag': 'X', '-2:postag[:2]': 'X', '-2:word.islower()': False, '+1:word.lower()': 'do', '+1:word.istitle()': False, '+1:word.isupper()': True, '+1:postag': 'ADP', '+1:postag[:2]': 'AD', '+1:word.islower()': False, '+2:word.lower()': 'parquet', '+2:word.istitle()': False, '+2:word.isupper()': True, '+2:postag': 'PROPN', '+2:postag[:2]': 'PR', '+2:word.islower()': False}, {'bias': 1.0, 'word.lower()': 'do', 'word[-3:]': 'DO', 'word[-2:]': 'DO', 'word.isupper()': True, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'ADP', 'postag[:2]': 'AD', 'word.islower()': False, 'word[0].isupper()': True, 'word[0].islower()': False, 'not word[0].isalnum()': False, 'not word.isalnum()': False, 'word.isalpha()': True, '-1:word.lower()': 'intervenção', '-1:word.istitle()': False, '-1:word.isupper()': True, '-1:postag': 'PROPN', '-1:postag[:2]': 'PR', '-1:word.islower()': False, '-2:word.lower()': 'de', '-2:word.istitle()': False, '-2:word.isupper()': True, '-2:postag': 'ADP', '-2:postag[:2]': 'AD', '-2:word.islower()': False, '+1:word.lower()': 'parquet', '+1:word.istitle()': False, '+1:word.isupper()': True, '+1:postag': 'PROPN', '+1:postag[:2]': 'PR', '+1:word.islower()': False, '+2:word.lower()': 'na', '+2:word.istitle()': False, '+2:word.isupper()': True, '+2:postag': 'ADP', '+2:postag[:2]': 'AD', '+2:word.islower()': False}, {'bias': 1.0, 'word.lower()': 'parquet', 'word[-3:]': 'UET', 'word[-2:]': 'ET', 'word.isupper()': True, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'PROPN', 'postag[:2]': 'PR', 'word.islower()': False, 'word[0].isupper()': True, 'word[0].islower()': False, 'not word[0].isalnum()': False, 'not word.isalnum()': False, 'word.isalpha()': True, '-1:word.lower()': 'do', '-1:word.istitle()': False, '-1:word.isupper()': True, '-1:postag': 'ADP', '-1:postag[:2]': 'AD', '-1:word.islower()': False, '-2:word.lower()': 'intervenção', '-2:word.istitle()': False, '-2:word.isupper()': True, '-2:postag': 'PROPN', '-2:postag[:2]': 'PR', '-2:word.islower()': False, '+1:word.lower()': 'na', '+1:word.istitle()': False, '+1:word.isupper()': True, '+1:postag': 'ADP', '+1:postag[:2]': 'AD', '+1:word.islower()': False, '+2:word.lower()': 'instância', '+2:word.istitle()': False, '+2:word.isupper()': True, '+2:postag': 'NOUN', '+2:postag[:2]': 'NO', '+2:word.islower()': False}, {'bias': 1.0, 'word.lower()': 'na', 'word[-3:]': 'NA', 'word[-2:]': 'NA', 'word.isupper()': True, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'ADP', 'postag[:2]': 'AD', 'word.islower()': False, 'word[0].isupper()': True, 'word[0].islower()': False, 'not word[0].isalnum()': False, 'not word.isalnum()': False, 'word.isalpha()': True, '-1:word.lower()': 'parquet', '-1:word.istitle()': False, '-1:word.isupper()': True, '-1:postag': 'PROPN', '-1:postag[:2]': 'PR', '-1:word.islower()': False, '-2:word.lower()': 'do', '-2:word.istitle()': False, '-2:word.isupper()': True, '-2:postag': 'ADP', '-2:postag[:2]': 'AD', '-2:word.islower()': False, '+1:word.lower()': 'instância', '+1:word.istitle()': False, '+1:word.isupper()': True, '+1:postag': 'NOUN', '+1:postag[:2]': 'NO', '+1:word.islower()': False, '+2:word.lower()': 'a', '+2:word.istitle()': True, '+2:word.isupper()': True, '+2:postag': 'DET', '+2:postag[:2]': 'DE', '+2:word.islower()': False}, {'bias': 1.0, 'word.lower()': 'instância', 'word[-3:]': 'CIA', 'word[-2:]': 'IA', 'word.isupper()': True, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'NOUN', 'postag[:2]': 'NO', 'word.islower()': False, 'word[0].isupper()': True, 'word[0].islower()': False, 'not word[0].isalnum()': False, 'not word.isalnum()': False, 'word.isalpha()': True, '-1:word.lower()': 'na', '-1:word.istitle()': False, '-1:word.isupper()': True, '-1:postag': 'ADP', '-1:postag[:2]': 'AD', '-1:word.islower()': False, '-2:word.lower()': 'parquet', '-2:word.istitle()': False, '-2:word.isupper()': True, '-2:postag': 'PROPN', '-2:postag[:2]': 'PR', '-2:word.islower()': False, '+1:word.lower()': 'a', '+1:word.istitle()': True, '+1:word.isupper()': True, '+1:postag': 'DET', '+1:postag[:2]': 'DE', '+1:word.islower()': False, '+2:word.lower()': 'quo', '+2:word.istitle()': False, '+2:word.isupper()': True, '+2:postag': 'PROPN', '+2:postag[:2]': 'PR', '+2:word.islower()': False}, {'bias': 1.0, 'word.lower()': 'a', 'word[-3:]': 'A', 'word[-2:]': 'A', 'word.isupper()': True, 'word.istitle()': True, 'word.isdigit()': False, 'postag': 'DET', 'postag[:2]': 'DE', 'word.islower()': False, 'word[0].isupper()': True, 'word[0].islower()': False, 'not word[0].isalnum()': False, 'not word.isalnum()': False, 'word.isalpha()': True, '-1:word.lower()': 'instância', '-1:word.istitle()': False, '-1:word.isupper()': True, '-1:postag': 'NOUN', '-1:postag[:2]': 'NO', '-1:word.islower()': False, '-2:word.lower()': 'na', '-2:word.istitle()': False, '-2:word.isupper()': True, '-2:postag': 'ADP', '-2:postag[:2]': 'AD', '-2:word.islower()': False, '+1:word.lower()': 'quo', '+1:word.istitle()': False, '+1:word.isupper()': True, '+1:postag': 'PROPN', '+1:postag[:2]': 'PR', '+1:word.islower()': False, '+2:word.lower()': '-', '+2:word.istitle()': False, '+2:word.isupper()': False, '+2:postag': 'PUNCT', '+2:postag[:2]': 'PU', '+2:word.islower()': False}, {'bias': 1.0, 'word.lower()': 'quo', 'word[-3:]': 'QUO', 'word[-2:]': 'UO', 'word.isupper()': True, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'PROPN', 'postag[:2]': 'PR', 'word.islower()': False, 'word[0].isupper()': True, 'word[0].islower()': False, 'not word[0].isalnum()': False, 'not word.isalnum()': False, 'word.isalpha()': True, '-1:word.lower()': 'a', '-1:word.istitle()': True, '-1:word.isupper()': True, '-1:postag': 'DET', '-1:postag[:2]': 'DE', '-1:word.islower()': False, '-2:word.lower()': 'instância', '-2:word.istitle()': False, '-2:word.isupper()': True, '-2:postag': 'NOUN', '-2:postag[:2]': 'NO', '-2:word.islower()': False, '+1:word.lower()': '-', '+1:word.istitle()': False, '+1:word.isupper()': False, '+1:postag': 'PUNCT', '+1:postag[:2]': 'PU', '+1:word.islower()': False, '+2:word.lower()': 'presença', '+2:word.istitle()': False, '+2:word.isupper()': True, '+2:postag': 'PROPN', '+2:postag[:2]': 'PR', '+2:word.islower()': False}, {'bias': 1.0, 'word.lower()': '-', 'word[-3:]': '-', 'word[-2:]': '-', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'PUNCT', 'postag[:2]': 'PU', 'word.islower()': False, 'word[0].isupper()': False, 'word[0].islower()': False, 'not word[0].isalnum()': True, 'not word.isalnum()': True, 'word.isalpha()': False, '-1:word.lower()': 'quo', '-1:word.istitle()': False, '-1:word.isupper()': True, '-1:postag': 'PROPN', '-1:postag[:2]': 'PR', '-1:word.islower()': False, '-2:word.lower()': 'a', '-2:word.istitle()': True, '-2:word.isupper()': True, '-2:postag': 'DET', '-2:postag[:2]': 'DE', '-2:word.islower()': False, '+1:word.lower()': 'presença', '+1:word.istitle()': False, '+1:word.isupper()': True, '+1:postag': 'PROPN', '+1:postag[:2]': 'PR', '+1:word.islower()': False, '+2:word.lower()': 'de', '+2:word.istitle()': False, '+2:word.isupper()': True, '+2:postag': 'ADP', '+2:postag[:2]': 'AD', '+2:word.islower()': False}, {'bias': 1.0, 'word.lower()': 'presença', 'word[-3:]': 'NÇA', 'word[-2:]': 'ÇA', 'word.isupper()': True, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'PROPN', 'postag[:2]': 'PR', 'word.islower()': False, 'word[0].isupper()': True, 'word[0].islower()': False, 'not word[0].isalnum()': False, 'not word.isalnum()': False, 'word.isalpha()': True, '-1:word.lower()': '-', '-1:word.istitle()': False, '-1:word.isupper()': False, '-1:postag': 'PUNCT', '-1:postag[:2]': 'PU', '-1:word.islower()': False, '-2:word.lower()': 'quo', '-2:word.istitle()': False, '-2:word.isupper()': True, '-2:postag': 'PROPN', '-2:postag[:2]': 'PR', '-2:word.islower()': False, '+1:word.lower()': 'de', '+1:word.istitle()': False, '+1:word.isupper()': True, '+1:postag': 'ADP', '+1:postag[:2]': 'AD', '+1:word.islower()': False, '+2:word.lower()': 'incapaz', '+2:word.istitle()': False, '+2:word.isupper()': True, '+2:postag': 'PROPN', '+2:postag[:2]': 'PR', '+2:word.islower()': False}, {'bias': 1.0, 'word.lower()': 'de', 'word[-3:]': 'DE', 'word[-2:]': 'DE', 'word.isupper()': True, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'ADP', 'postag[:2]': 'AD', 'word.islower()': False, 'word[0].isupper()': True, 'word[0].islower()': False, 'not word[0].isalnum()': False, 'not word.isalnum()': False, 'word.isalpha()': True, '-1:word.lower()': 'presença', '-1:word.istitle()': False, '-1:word.isupper()': True, '-1:postag': 'PROPN', '-1:postag[:2]': 'PR', '-1:word.islower()': False, '-2:word.lower()': '-', '-2:word.istitle()': False, '-2:word.isupper()': False, '-2:postag': 'PUNCT', '-2:postag[:2]': 'PU', '-2:word.islower()': False, '+1:word.lower()': 'incapaz', '+1:word.istitle()': False, '+1:word.isupper()': True, '+1:postag': 'PROPN', '+1:postag[:2]': 'PR', '+1:word.islower()': False, '+2:word.lower()': '-', '+2:word.istitle()': False, '+2:word.isupper()': False, '+2:postag': 'PUNCT', '+2:postag[:2]': 'PU', '+2:word.islower()': False}, {'bias': 1.0, 'word.lower()': 'incapaz', 'word[-3:]': 'PAZ', 'word[-2:]': 'AZ', 'word.isupper()': True, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'PROPN', 'postag[:2]': 'PR', 'word.islower()': False, 'word[0].isupper()': True, 'word[0].islower()': False, 'not word[0].isalnum()': False, 'not word.isalnum()': False, 'word.isalpha()': True, '-1:word.lower()': 'de', '-1:word.istitle()': False, '-1:word.isupper()': True, '-1:postag': 'ADP', '-1:postag[:2]': 'AD', '-1:word.islower()': False, '-2:word.lower()': 'presença', '-2:word.istitle()': False, '-2:word.isupper()': True, '-2:postag': 'PROPN', '-2:postag[:2]': 'PR', '-2:word.islower()': False, '+1:word.lower()': '-', '+1:word.istitle()': False, '+1:word.isupper()': False, '+1:postag': 'PUNCT', '+1:postag[:2]': 'PU', '+1:word.islower()': False, '+2:word.lower()': 'prejuízo', '+2:word.istitle()': False, '+2:word.isupper()': True, '+2:postag': 'PROPN', '+2:postag[:2]': 'PR', '+2:word.islower()': False}, {'bias': 1.0, 'word.lower()': '-', 'word[-3:]': '-', 'word[-2:]': '-', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'PUNCT', 'postag[:2]': 'PU', 'word.islower()': False, 'word[0].isupper()': False, 'word[0].islower()': False, 'not word[0].isalnum()': True, 'not word.isalnum()': True, 'word.isalpha()': False, '-1:word.lower()': 'incapaz', '-1:word.istitle()': False, '-1:word.isupper()': True, '-1:postag': 'PROPN', '-1:postag[:2]': 'PR', '-1:word.islower()': False, '-2:word.lower()': 'de', '-2:word.istitle()': False, '-2:word.isupper()': True, '-2:postag': 'ADP', '-2:postag[:2]': 'AD', '-2:word.islower()': False, '+1:word.lower()': 'prejuízo', '+1:word.istitle()': False, '+1:word.isupper()': True, '+1:postag': 'PROPN', '+1:postag[:2]': 'PR', '+1:word.islower()': False, '+2:word.lower()': 'existente', '+2:word.istitle()': False, '+2:word.isupper()': True, '+2:postag': 'PROPN', '+2:postag[:2]': 'PR', '+2:word.islower()': False}, {'bias': 1.0, 'word.lower()': 'prejuízo', 'word[-3:]': 'ÍZO', 'word[-2:]': 'ZO', 'word.isupper()': True, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'PROPN', 'postag[:2]': 'PR', 'word.islower()': False, 'word[0].isupper()': True, 'word[0].islower()': False, 'not word[0].isalnum()': False, 'not word.isalnum()': False, 'word.isalpha()': True, '-1:word.lower()': '-', '-1:word.istitle()': False, '-1:word.isupper()': False, '-1:postag': 'PUNCT', '-1:postag[:2]': 'PU', '-1:word.islower()': False, '-2:word.lower()': 'incapaz', '-2:word.istitle()': False, '-2:word.isupper()': True, '-2:postag': 'PROPN', '-2:postag[:2]': 'PR', '-2:word.islower()': False, '+1:word.lower()': 'existente', '+1:word.istitle()': False, '+1:word.isupper()': True, '+1:postag': 'PROPN', '+1:postag[:2]': 'PR', '+1:word.islower()': False, '+2:word.lower()': '-', '+2:word.istitle()': False, '+2:word.isupper()': False, '+2:postag': 'PUNCT', '+2:postag[:2]': 'PU', '+2:word.islower()': False}, {'bias': 1.0, 'word.lower()': 'existente', 'word[-3:]': 'NTE', 'word[-2:]': 'TE', 'word.isupper()': True, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'PROPN', 'postag[:2]': 'PR', 'word.islower()': False, 'word[0].isupper()': True, 'word[0].islower()': False, 'not word[0].isalnum()': False, 'not word.isalnum()': False, 'word.isalpha()': True, '-1:word.lower()': 'prejuízo', '-1:word.istitle()': False, '-1:word.isupper()': True, '-1:postag': 'PROPN', '-1:postag[:2]': 'PR', '-1:word.islower()': False, '-2:word.lower()': '-', '-2:word.istitle()': False, '-2:word.isupper()': False, '-2:postag': 'PUNCT', '-2:postag[:2]': 'PU', '-2:word.islower()': False, '+1:word.lower()': '-', '+1:word.istitle()': False, '+1:word.isupper()': False, '+1:postag': 'PUNCT', '+1:postag[:2]': 'PU', '+1:word.islower()': False, '+2:word.lower()': 'preliminar', '+2:word.istitle()': False, '+2:word.isupper()': True, '+2:postag': 'PROPN', '+2:postag[:2]': 'PR', '+2:word.islower()': False}, {'bias': 1.0, 'word.lower()': '-', 'word[-3:]': '-', 'word[-2:]': '-', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'PUNCT', 'postag[:2]': 'PU', 'word.islower()': False, 'word[0].isupper()': False, 'word[0].islower()': False, 'not word[0].isalnum()': True, 'not word.isalnum()': True, 'word.isalpha()': False, '-1:word.lower()': 'existente', '-1:word.istitle()': False, '-1:word.isupper()': True, '-1:postag': 'PROPN', '-1:postag[:2]': 'PR', '-1:word.islower()': False, '-2:word.lower()': 'prejuízo', '-2:word.istitle()': False, '-2:word.isupper()': True, '-2:postag': 'PROPN', '-2:postag[:2]': 'PR', '-2:word.islower()': False, '+1:word.lower()': 'preliminar', '+1:word.istitle()': False, '+1:word.isupper()': True, '+1:postag': 'PROPN', '+1:postag[:2]': 'PR', '+1:word.islower()': False, '+2:word.lower()': 'acolhida', '+2:word.istitle()': False, '+2:word.isupper()': True, '+2:postag': 'PROPN', '+2:postag[:2]': 'PR', '+2:word.islower()': False}, {'bias': 1.0, 'word.lower()': 'preliminar', 'word[-3:]': 'NAR', 'word[-2:]': 'AR', 'word.isupper()': True, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'PROPN', 'postag[:2]': 'PR', 'word.islower()': False, 'word[0].isupper()': True, 'word[0].islower()': False, 'not word[0].isalnum()': False, 'not word.isalnum()': False, 'word.isalpha()': True, '-1:word.lower()': '-', '-1:word.istitle()': False, '-1:word.isupper()': False, '-1:postag': 'PUNCT', '-1:postag[:2]': 'PU', '-1:word.islower()': False, '-2:word.lower()': 'existente', '-2:word.istitle()': False, '-2:word.isupper()': True, '-2:postag': 'PROPN', '-2:postag[:2]': 'PR', '-2:word.islower()': False, '+1:word.lower()': 'acolhida', '+1:word.istitle()': False, '+1:word.isupper()': True, '+1:postag': 'PROPN', '+1:postag[:2]': 'PR', '+1:word.islower()': False, '+2:word.lower()': '-', '+2:word.istitle()': False, '+2:word.isupper()': False, '+2:postag': 'PUNCT', '+2:postag[:2]': 'PU', '+2:word.islower()': False}, {'bias': 1.0, 'word.lower()': 'acolhida', 'word[-3:]': 'IDA', 'word[-2:]': 'DA', 'word.isupper()': True, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'PROPN', 'postag[:2]': 'PR', 'word.islower()': False, 'word[0].isupper()': True, 'word[0].islower()': False, 'not word[0].isalnum()': False, 'not word.isalnum()': False, 'word.isalpha()': True, '-1:word.lower()': 'preliminar', '-1:word.istitle()': False, '-1:word.isupper()': True, '-1:postag': 'PROPN', '-1:postag[:2]': 'PR', '-1:word.islower()': False, '-2:word.lower()': '-', '-2:word.istitle()': False, '-2:word.isupper()': False, '-2:postag': 'PUNCT', '-2:postag[:2]': 'PU', '-2:word.islower()': False, '+1:word.lower()': '-', '+1:word.istitle()': False, '+1:word.isupper()': False, '+1:postag': 'PUNCT', '+1:postag[:2]': 'PU', '+1:word.islower()': False, '+2:word.lower()': 'nulidade', '+2:word.istitle()': False, '+2:word.isupper()': True, '+2:postag': 'PROPN', '+2:postag[:2]': 'PR', '+2:word.islower()': False}, {'bias': 1.0, 'word.lower()': '-', 'word[-3:]': '-', 'word[-2:]': '-', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'PUNCT', 'postag[:2]': 'PU', 'word.islower()': False, 'word[0].isupper()': False, 'word[0].islower()': False, 'not word[0].isalnum()': True, 'not word.isalnum()': True, 'word.isalpha()': False, '-1:word.lower()': 'acolhida', '-1:word.istitle()': False, '-1:word.isupper()': True, '-1:postag': 'PROPN', '-1:postag[:2]': 'PR', '-1:word.islower()': False, '-2:word.lower()': 'preliminar', '-2:word.istitle()': False, '-2:word.isupper()': True, '-2:postag': 'PROPN', '-2:postag[:2]': 'PR', '-2:word.islower()': False, '+1:word.lower()': 'nulidade', '+1:word.istitle()': False, '+1:word.isupper()': True, '+1:postag': 'PROPN', '+1:postag[:2]': 'PR', '+1:word.islower()': False, '+2:word.lower()': 'reconhecida', '+2:word.istitle()': False, '+2:word.isupper()': True, '+2:postag': 'NOUN', '+2:postag[:2]': 'NO', '+2:word.islower()': False}, {'bias': 1.0, 'word.lower()': 'nulidade', 'word[-3:]': 'ADE', 'word[-2:]': 'DE', 'word.isupper()': True, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'PROPN', 'postag[:2]': 'PR', 'word.islower()': False, 'word[0].isupper()': True, 'word[0].islower()': False, 'not word[0].isalnum()': False, 'not word.isalnum()': False, 'word.isalpha()': True, '-1:word.lower()': '-', '-1:word.istitle()': False, '-1:word.isupper()': False, '-1:postag': 'PUNCT', '-1:postag[:2]': 'PU', '-1:word.islower()': False, '-2:word.lower()': 'acolhida', '-2:word.istitle()': False, '-2:word.isupper()': True, '-2:postag': 'PROPN', '-2:postag[:2]': 'PR', '-2:word.islower()': False, '+1:word.lower()': 'reconhecida', '+1:word.istitle()': False, '+1:word.isupper()': True, '+1:postag': 'NOUN', '+1:postag[:2]': 'NO', '+1:word.islower()': False, '+2:word.lower()': '.', '+2:word.istitle()': False, '+2:word.isupper()': False, '+2:postag': 'PUNCT', '+2:postag[:2]': 'PU', '+2:word.islower()': False}, {'bias': 1.0, 'word.lower()': 'reconhecida', 'word[-3:]': 'IDA', 'word[-2:]': 'DA', 'word.isupper()': True, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'NOUN', 'postag[:2]': 'NO', 'word.islower()': False, 'word[0].isupper()': True, 'word[0].islower()': False, 'not word[0].isalnum()': False, 'not word.isalnum()': False, 'word.isalpha()': True, '-1:word.lower()': 'nulidade', '-1:word.istitle()': False, '-1:word.isupper()': True, '-1:postag': 'PROPN', '-1:postag[:2]': 'PR', '-1:word.islower()': False, '-2:word.lower()': '-', '-2:word.istitle()': False, '-2:word.isupper()': False, '-2:postag': 'PUNCT', '-2:postag[:2]': 'PU', '-2:word.islower()': False, '+1:word.lower()': '.', '+1:word.istitle()': False, '+1:word.isupper()': False, '+1:postag': 'PUNCT', '+1:postag[:2]': 'PU', '+1:word.islower()': False}, {'bias': 1.0, 'word.lower()': '.', 'word[-3:]': '.', 'word[-2:]': '.', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'PUNCT', 'postag[:2]': 'PU', 'word.islower()': False, 'word[0].isupper()': False, 'word[0].islower()': False, 'not word[0].isalnum()': True, 'not word.isalnum()': True, 'word.isalpha()': False, '-1:word.lower()': 'reconhecida', '-1:word.istitle()': False, '-1:word.isupper()': True, '-1:postag': 'NOUN', '-1:postag[:2]': 'NO', '-1:word.islower()': False, '-2:word.lower()': 'nulidade', '-2:word.istitle()': False, '-2:word.isupper()': True, '-2:postag': 'PROPN', '-2:postag[:2]': 'PR', '-2:word.islower()': False, 'EOS': True}]\n",
            "\n",
            "Label: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORGANIZACAO', 'I-ORGANIZACAO', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "crf = CRF(max_iterations=100, c1=0.1, c2=0.1, all_possible_transitions=True)\n",
        "\n",
        "# Testar com max_iterations=100 e all_possible_transitions=True"
      ],
      "metadata": {
        "id": "4hCHY0F08sr6"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  crf.fit(X_train, y_train)\n",
        "except AttributeError:\n",
        "  pass"
      ],
      "metadata": {
        "id": "RQVqdK6l87wC"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = crf.predict(X_test)"
      ],
      "metadata": {
        "id": "9vlds6NBhjEf"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "print(report)\n"
      ],
      "metadata": {
        "id": "3YlEN-vNh6Lo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e11d79c-2c6c-4e29-a296-c7b10d9cba97"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                precision    recall  f1-score   support\n",
            "\n",
            "JURISPRUDENCIA       0.75      0.71      0.73       185\n",
            "    LEGISLACAO       0.84      0.80      0.82       378\n",
            "         LOCAL       0.65      0.66      0.65        47\n",
            "   ORGANIZACAO       0.89      0.78      0.83       501\n",
            "        PESSOA       0.90      0.76      0.83       233\n",
            "         TEMPO       0.92      0.78      0.84       192\n",
            "\n",
            "     micro avg       0.86      0.77      0.81      1536\n",
            "     macro avg       0.82      0.75      0.78      1536\n",
            "  weighted avg       0.86      0.77      0.81      1536\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from io import StringIO\n",
        "\n",
        "report = classification_report(y_test, y_pred, output_dict=True)\n",
        "results = []\n",
        "for categoria, metrics in report.items():\n",
        "    # Skip 'accuracy', 'macro avg', and 'weighted avg' keys\n",
        "    if categoria in ('micro avg', 'macro avg', 'weighted avg'):\n",
        "        continue\n",
        "\n",
        "    precision = metrics['precision']\n",
        "    recall = metrics['recall']\n",
        "    f1_score = metrics['f1-score']\n",
        "    support = metrics['support']\n",
        "    results.append([categoria, precision, recall, f1_score, support])\n",
        "\n",
        "df_results = pd.DataFrame(results, columns=['Categoria', 'Precisão', 'Recall', 'F1-Score', 'Support'])\n",
        "arquivo_csv = f'PLN_TRABALHO-02-{corpus_name}.csv'\n",
        "df_results.to_csv(arquivo_csv, index=False, encoding='utf-8')\n",
        "\n",
        "print(\"CSV file \"+arquivo_csv+\" created successfully.\")"
      ],
      "metadata": {
        "id": "xQYEWOTu39c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07cd40a1-1f7d-4339-cf03-583e4ae5034e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV file PLN_TRABALHO-02-le_ner.csv created successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_ner(x, y, exclude=None, is_bio=False):\n",
        "    if len(x) != len(y):\n",
        "        print(\"\\nERROR: 'x' and 'y' must be the same size.\")\n",
        "        return None\n",
        "    if type(exclude) is not list:\n",
        "        exclude = []  # Guarda os tokens que serão excluídos do resultado\n",
        "    extracted_ners = []  # Guarda dicionários com as entidades nomeadas filtradas/agrupadas\n",
        "    for i in range(len(y)):\n",
        "        ind_last_token = len(y[i]) - 1\n",
        "        j = 0\n",
        "        same_entity = []  # Guarda os índices dos tokens vizinhos que pertencem à mesma entidade nomeada\n",
        "        ners = {}  # Guarda o índice do token (como chave) e uma lista de tokens vizinhos que pertencem à mesma entidade\n",
        "        has_began = False  # Indica se um label começou com 'B-', no caso do formato BIO\n",
        "        # Percorre cada um dos tipos de tokens da sentença e, se houver, verifica se o proximo tipo de token é igual\n",
        "        while j <= ind_last_token:\n",
        "            if y[i][j] not in exclude:\n",
        "                if j not in same_entity:\n",
        "                    same_entity.append(j)\n",
        "                    if is_bio and len(y[i][j]) >= 2 and y[i][j][:2].upper() == 'B-':  # Se é do formato BIO e é início\n",
        "                        has_began = True\n",
        "                if j + 1 <= ind_last_token:\n",
        "                    if has_began and len(y[i][j + 1]) >= 2 and y[i][j + 1][:2].upper() == 'I-' \\\n",
        "                            and y[i][j][2:] == y[i][j + 1][2:]:\n",
        "                        same_entity.append(j + 1)\n",
        "                    elif not is_bio and y[i][j] == y[i][j + 1]:\n",
        "                        same_entity.append(j + 1)\n",
        "                    else:\n",
        "                        ners[j] = same_entity\n",
        "                        same_entity = []\n",
        "                        has_began = False\n",
        "                else:\n",
        "                    ners[j] = same_entity\n",
        "                    has_began = False\n",
        "            j += 1\n",
        "        # Processa as ners extraidas da sentença desta iteração\n",
        "        if ners:\n",
        "            sent = x[i]\n",
        "            ners_aux = []\n",
        "            for ind_token, filtered_tokens in ners.items():\n",
        "                ner = ''\n",
        "                for t in filtered_tokens:\n",
        "                    ner += sent[t] + ' '\n",
        "                # Trata o caso do formato BIO e retira os 'B-' e 'I-' dos nomes dos labels\n",
        "                if is_bio and len(y[i][ind_token]) >= 2:\n",
        "                    label = y[i][ind_token][2:]\n",
        "                else:\n",
        "                    label = y[i][ind_token]\n",
        "                ners_aux.append((ner.strip(), label))\n",
        "            extracted_ners.append(ners_aux)\n",
        "        else:\n",
        "            extracted_ners.append([])  # Para manter a ordem das sentenças, caso não seja extraida nenhuma ner\n",
        "    return extracted_ners"
      ],
      "metadata": {
        "id": "GBGCw6xNh7Ng"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ner_ext_test = extract_ner(test_data_original[:, 0],test_data_original[:, 1], exclude=['O'], is_bio=True)\n",
        "\n",
        "ner_ext_pred = extract_ner(test_data_original[:, 0], y_pred, exclude=['O'], is_bio=True)"
      ],
      "metadata": {
        "id": "zT0EUwfkmFWb"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ner_ext_test[0]"
      ],
      "metadata": {
        "id": "5nyO8De9mZJi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9480de4-7b5e-4065-98ea-2d8d541e0d39"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('ACÓRDÃO 1160/2016', 'JURISPRUDENCIA'),\n",
              " ('PLENÁRIO', 'ORGANIZACAO'),\n",
              " ('AUGUSTO NARDES', 'PESSOA'),\n",
              " ('Processo 006.010/2000-4', 'JURISPRUDENCIA'),\n",
              " ('11/05/2016', 'TEMPO'),\n",
              " ('Ana Arraes', 'PESSOA')]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ner_ext_pred[0]"
      ],
      "metadata": {
        "id": "umcHawyfnh6S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b45ac839-3aca-478a-9c04-500c7b01c256"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('ACÓRDÃO 1160/2016', 'JURISPRUDENCIA'),\n",
              " ('PLENÁRIO', 'ORGANIZACAO'),\n",
              " ('AUGUSTO NARDES', 'PESSOA'),\n",
              " ('Processo 006.010/2000-4', 'JURISPRUDENCIA'),\n",
              " ('11/05/2016', 'TEMPO'),\n",
              " ('Ana Arraes', 'PESSOA')]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    }
  ]
}